---
title: "EDDA Assignment 1"
author: "Agnes Nikki Tim, group 38"
date: "13 February 2023"
output: pdf_document
fontsize: 11pt
highlight: tango
---

# Exercise 2: **Expenditure on criminal activities**

### The data in [expensescrime.txt](https://canvas.vu.nl/courses/68030/files/6051345?wrap=1 "expensescrime.txt") were obtained to determine factors related to state expenditures on criminal activities (courts, police, etc.) The variables are: *state* (indicating the state in the USA), *expend* (state expenditures on criminal activities in \$1000), *bad* (crime rate per 100000), *crime* (number of persons under criminal supervision), *lawyers* (number of lawyers in the state), *employ* (number of persons employed in the state) and *pop* (population of the state in 1000). In the regression analysis, take *expend* as response variable and *bad*, *crime*, *lawyers*, *employ* and *pop* as explanatory variables.

## A

### Make some graphical summaries of the data.

```{r, echo=F}
expenses = read.table("expensescrime.txt", header=T)
```

The scatterplot matrix below was created to visualize the pairwise relationships between the explanatory variables. In this graph, a high collinearity between two variables corresponds with a linear relationship in the respective scatterplot.

```{r, echo=F}
pairs(expenses[, c("expend", "bad", "crime", "lawyers", "employ", "pop")])
```

Based on the pairs plot, it appears that the following variables may be correlated:

-   Expend - bad

-   Expend - lawyers

-   Expend - employ

-   Expend - pop

-   Bad - lawyers

-   Bad - employ

-   Bad - pop

-   Lawyers - employ

-   Lawyers - pop

-   Employ - pop

To check the assumptions for the model with all explanatory variables, the residuals should look normal and their spread should not vary with the fitted values.

```{r, echo=F}
model <- lm(expend ~ bad + crime + lawyers + employ + pop, data = expenses)
par(mfrow = c(1, 2))
plot(residuals(model), expenses$expend)
qqnorm(residuals(model))
```

The residuals are not normally distributed. Therefore we discard the expenses \~ bad model.

### Investigate the problem of influence point

For each observation, the Cook's distance was determined to investigated the problem of influential points. Points with a Cook's distance larger than 1 are considered to be influence points (see plot below).

```{r, echo=F}
dist = cooks.distance(model)
```

```{r, echo=F}
plot(dist, pch = 19, type="b", xlab = "Observation", ylab = "Cook's Distance", main = "Cook's Distances")
```

### Investigate the problem of collinearity

Collinearity is the problem of linear relations between explanatory variables. The correlation matrix of the independent variables was calculated to investigate the issue.

```{r, echo=F}
cor(expenses[, c("bad", "crime", "lawyers", "employ", "pop")])
```

Moreover, the Variance Inflation Factors (VIF) were examined.

```{r, echo=F}
library(car); vif(model)
```

If a VIF value is larger than 5, this gives reason for concern. The output suggests that there is a collinearity problem for the variables bad, lawyers, employ and pop.

### Fit a linear regression model to the data. Use the step-up method to find the best model. Comment

In the step-up technique predictors are sequentially added one at a time. The first variable to was chosen by finding the explanatory variable with the highest multiple R-squared value.

```{r, echo=F, eval=F}
summary(lm(expend ~ bad, data=expenses))
summary(lm(expend ~ crime, data=expenses))
summary(lm(expend ~ lawyers, data=expenses))
summary(lm(expend ~ employ, data=expenses))
summary(lm(expend ~ pop, data=expenses))
```

Employ is selected with

    Multiple R-squared:  0.954

```{r, echo=F, eval=F}
summary(lm(expend ~ employ + bad, data=expenses))
summary(lm(expend ~ employ + crime, data=expenses))
summary(lm(expend ~ employ + lawyers, data=expenses))
summary(lm(expend ~ employ + pop, data=expenses))
```

The variable lawyers is added to the model with

    Multiple R-squared:  0.9632

None of the variables is significant when adding the remaining variables, so the resulting model only contains employ and lawyer. There may be collinearity among the explanatory variables, and one should actually avoid having two collinear explanatory variables in a model. This is something to keep in mind when tweaking the model further.

```{r, echo=F}
step_up = (lm(expend ~ employ + lawyers, data=expenses))
```

### Determine a 95% prediction interval for the *expend* using the model you preferred in b) for a (hypothetical) state with *bad*=50, *crime*=5000, *lawyers*=5000, *employ*=5000 and *pop*=5000.

```{r, echo=F}
newdata <- data.frame(bad = 50, crime = 5000, lawyers = 5000, employ = 5000, pop = 5000)
pred <- predict(step_up, newdata, interval = "prediction", level = 0.95)
pred
```

### Can you improve this interval?

Yes, perhaps by removing influence points.

### Apply the LASSO method to choose the relevant variables (with default parameters as in the lecture and *lambda*=*lambda.1se*)

Lasso automatically shrink the coefficients of the insignificant variables or sets them to zero altogether.

```{r, echo=F}
library(glmnet)
library(caret)
set.seed(1)

# divide into train and test data
train_index = createDataPartition(expenses$expend, p = 2/3, list = FALSE)
train_data = expenses[train_index, ]
test_data = expenses[-train_index, ]

# select explanatory variables
indep_vars = c("bad", "crime", "lawyers", "employ", "pop")
x_train = train_data[indep_vars]
x_train = data.matrix(x_train)
y_train = train_data$expend

x_test = test_data[indep_vars]
x_test = data.matrix(x_test)

y_test = test_data$expend
```

```{r}
lasso_mod = glmnet(x_train, y_train, alpha = 1)
plot(lasso_mod, label=T, xvar="lambda")
cv_lasso = cv.glmnet(x_train, y_train, alpha=1, type.measure="mse")
```

```{r, echo=F}
lambda_min = cv_lasso$lambda_min
lambda_1se = cv_lasso$lambda.1se
coeficients = coef(lasso_mod, s=cv_lasso$lambda_min)
```

Variables with coefficients close to zero indicates that these were not deemed relevant for predicting expenditure by the LASSO algorithm.

### Compare the resulting model with the model obtained in b)

```{r, echo=F, eval=F}
y_pred_lasso = predict(lasso_mod, s=lambda_min, newx = x_test) 
mse_lasso = mean((y_test - y_pred_lasso)^2)
mse_lasso

mse <- function(sm) 
    mean(sm$residuals^2)

mse(model)
```

The LASSO model was found to have a higher root mean squared error (293768.6) than the model obtained in b (44896.39). This implies that
