---
title: "EDDA Assignment 1"
author: "Agnes Nikki Tim, group 38"
date: "13 February 2023"
output: pdf_document
fontsize: 11pt
highlight: tango
---

# Exercise 2: **Expenditure on criminal activities**

### The data in [expensescrime.txt](https://canvas.vu.nl/courses/68030/files/6051345?wrap=1 "expensescrime.txt")were obtained to determine factors related to state expenditures on criminal activities (courts, police, etc.) The variables are: *state* (indicating the state in the USA), *expend* (state expenditures on criminal activities in \$1000), *bad* (crime rate per 100000), *crime* (number of persons under criminal supervision), *lawyers* (number of lawyers in the state), *employ* (number of persons employed in the state) and *pop* (population of the state in 1000). In the regression analysis, take *expend* as response variable and *bad*, *crime*, *lawyers*, *employ* and *pop* as explanatory variables.

## A

### Make some graphical summaries of the data.

```{r}
expenses = read.table("expensescrime.txt", header=T)
expenses
```

The scatterplot matrix below was created to visualize the pairwise relationships between the explanatory variables. In this graph, a high collinearity between two variables corresponds with a linear relationship in the respective scatterplot.

```{r}
pairs(expenses[, c("expend", "bad", "crime", "lawyers", "employ", "pop")])
```

Based on the pairs plot, it appears that the following variables may be correlated:

-   Expend - bad

-   Expend - lawyers

-   Expend - employ

-   Expend - pop

-   Bad - lawyers

-   Bad - employ

-   Bad - pop

-   Lawyers - employ

-   Lawyers - pop

-   Employ - pop

```{r}
expend_bad_lm = lm(expend ~ bad, data=expenses)
plot(residuals(expend_bad_lm), expenses$bad)
```

To check the assumptions the residuals should look normal and their spread should not vary with the
fitted values.

```{r}
qqnorm(residuals(expend_bad_lm))
```

The residuals are not normally distributed. Therefore we discard the expenses \~ bad model.

### Investigate the problem of influence points

```{r}
model <- lm(expend ~ bad + crime + lawyers + employ + pop, data = expenses)
influence = influence.measures(model)
influence
```

```{r}
plot(influence$c, pch = 20, main = "Cook's Distance")
```

For each observation, the Cook's distance was used to investigated the problem of influential points. Points with high Cook's distance are potential influence points.

```{r}
dist = cooks.distance(model)
dist
```

```{r}
plot(dist, pch = 19, type="b", xlab = "Observation", ylab = "Cook's Distance", main = "Cook's Distances")
```

### Investigate the problem of collinearity.

Collinearity is the problem of linear relations between explanatory variables. The correlation matrix of the independent variables was calculated to investigate the issue.

```{r}
cor(expenses[, c("bad", "crime", "lawyers", "employ", "pop")])
```

Moreover, the Variance Inflation Factors (VIF) were examined.

```{r}
library(car); vif(model)
```

If a VIF value is larger than 5, this gives reason for concern. The output suggests that there is a collinearity problem for the variables bad, lawyers, employ and pop.

### Fit a linear regression model to the data. Use the step-up method to find the best model. Comment

In the step-up technique predictors are sequentially added one at a time. The first variable to was chosen by finding the explanatory variable with the highest multiple R-squared value.

```{r}
summary(lm(expend ~ bad, data=expenses))
summary(lm(expend ~ crime, data=expenses))
summary(lm(expend ~ lawyers, data=expenses))
summary(lm(expend ~ employ, data=expenses))
summary(lm(expend ~ pop, data=expenses))
```

Employ is selected with

    Multiple R-squared:  0.954

```{r}
summary(lm(expend ~ employ + bad, data=expenses))
summary(lm(expend ~ employ + crime, data=expenses))
summary(lm(expend ~ employ + lawyers, data=expenses))
summary(lm(expend ~ employ + pop, data=expenses))
```

The variable lawyers is added to the model with

    Multiple R-squared:  0.9632

```{r}
summary(lm(expend ~ employ + lawyers + bad, data=expenses))
summary(lm(expend ~ employ + lawyers + crime, data=expenses))
summary(lm(expend ~ employ + lawyers + pop, data=expenses))
```

The variable bad is added to the model with

    Multiple R-squared:  0.9639

```{r}
 summary(lm(expend ~ employ + lawyers + bad, data=expenses))

```

CONTINUE HERE

```{r}
step_up = 
```

### Determine a 95% prediction interval for the *expend* using the model you preferred in b) for a (hypothetical) state with *bad*=50, *crime*=5000, *lawyers*=5000, *employ*=5000 and *pop*=5000. Can you improve this interval?

```{r}
newdata <- data.frame(bad = 50, crime = 5000, lawyers = 5000, employ = 5000, pop = 5000)
pred <- predict(step_up, newdata, interval = "prediction", level = 0.95)
```

### Apply the LASSO method to choose the relevant variables (with default parameters as in the lecture and *lambda*=*lambda.1se*). (You will need to install the R-package *glmnet*, which is not included in the standard distribution of R.) Compare the resulting model with the model obtained in b). (Beware that in general a new run delivers a new model because of a new train set.)

```{r}
library(glmnet)
library(caret)
set.seed(1)

# divide into train and test data
train_index = createDataPartition(expenses$expend, p = 2/3, list = FALSE)
train_data = expenses[train_index, ]
test_data = expenses[-train_index, ]

# select explanatory variables
indep_vars = c("bad", "crime", "lawyers", "employ", "pop")
x_train = train_data[indep_vars]
y_train = train_data$expend


lasso_mod = glmnet(x_train, y_train, alpha = 1)
cv_lasso = cv.glmnet(x_train, y_train, alpha=1, type.measure="mse")
plot(lasso_mod, label=T, xvar="lambda")
```

```{r}
lambda_min = lasso.cv$lambda.min; lambda_1se = lasso.cv$lambda.1se
coef(lasso.model, s=lasso.cv$lambda.min) 
y.pred=predict(lasso.model,s=lambda.min,newx=x.test) 
# calculate MSE
mse.lasso=mean((y.test-y.pred)^2)
```
